\section{Introduction}\label{section:introduction}

An infix operator $\oplus:S\times S\rightarrow S$ is associative iff.
\[\forall\ x,y,z\in S\ .\ \p{x \oplus y} \oplus z = x \oplus \p{y \oplus z}.\]

One of the benefits of such functions is that a sequence of values
$\p{x_k}_{k=1}^n,\ x_k\in S$, can be aggregated into a single value $x\in S$,
using $n-1$ applications of $\oplus$, i.e. $x = x_1 \oplus \cdots \oplus
x_{n-1} \oplus x_n$. This is more commonly known as a \emph{reduction}.  For
instance, we can compute the sum of $n$ integers using $n-1$ binary additions.

A straight-forward reduction strategy is $\p{\cdots \p{\p{x_1 \oplus x_2}
\oplus x_3} \cdots \oplus x_{n-1}} \oplus x_n$, or equivalently, $x_1 \oplus
\p{x_2 \oplus \cdots \p{x_{n-2} \oplus \p{x_{n-1} \oplus x_n}} \cdots }$.  This
is known as a linear left or right \emph{fold}, respectively.

Reductions get more interesting on single-instruction multiple data (SIMD)
architectures. A reduction on a SIMD machine can be performed in parallel time
$O\p{\log n}$ on $n$ processors, or ${n/m}+O\p{\log m}$ on $m$ processors,
using a technique called \emph{parallel prefix scan}, or simply scan.

Scan is an interesting technique in and of itself as it can serve as a basic
building block in the design of parallel algorithms, replacing primitive
parallel memory referencing. If applicable, this technique typically reduces
the asymptotic bounds by a factor of $O\p{\log n}$\cite{blelloch}.

Scan takes an infix operator $\oplus:S\times S\rightarrow S$, an identity value
$i\in S$, such that $i\oplus i = i$, and a sequence $\p{x_k}_{k=1}^n,\ x_k\in
S$.  It produces the sequence $\p{i, x_1, x_1 \oplus x_2, \ldots, x_1 \oplus
\cdots \oplus x_{n-1} \oplus x_n}$.

To do this, parallel prefix scan performs a tree-like reduction rather than a
linear reduction. The sequence $\p{x_i \oplus x_{i+1}}_{i=1}^{n-1}$ is computed
first, by pairwise applying $\oplus$ to the elements of the sequence. The
sequence $\p{\p{x_i \oplus x_{i+1}} \oplus \p{x_{i+2} \oplus
x_{i+3}}}_{i=1}^{n-3}$ is computed second, by pairwise applying $\oplus$ to the
elements of the resulting sequence, and so on until a scalar is reached.

It is important for the correctness of the method that the function supplied to
scan is an associative function. The requirements could be relaxed to making
sure the function susceptible to tree-like reduction, but that would render the
functions identified by this method not necessarily suitable for linear
reductions. This would in turn discourage us from applying transformations to
turn linear reductions into tree-like reductions: we could speed up the program
by using parallel prefix scans rather than folds.

Most compilers will assume that the user has supplied an associative function.
Others will require for the function to be explicitly marked as
``associative''. Others still will try to deduce the associativity property,
and warn the user if the function isn't clearly associative. The last is
clearly preferable, especially for scientific computing languages such as L0,
since such a bug can be very hard to find (the user might not be aware that
associativity is a requirement at all).

L0 is an eagerly evaluated, purely functional, second order, array programming
language. The language encourages the use of arrays and built-in second-order
array combinators for all practical problems. L0 has a focus on efficient
execution on vector hardware.

Section 2 provides a proof of the undecidability of the associative property.
Section 3 defines the associative type, expanding the definition of
associativity in general. Section 4 covers the built-in operators in L0, and
whether each of them is associative under chaining. Section 5 covers Light's
associativity test which allows testing the associativity of functions built
solely from bitwise operators. Section 5 also suggests using a light-weight
Light's associativity test in a manner known as property-based testing. This
technique works for arbitrary functions. Section 6 covers a more
straight-forward approach, based on rewriting. Section 7 describes the overall
algorithm we suggest. Section 8 discusses possible future work. Section 9
concludes the paper.

